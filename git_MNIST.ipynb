{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "np.set_printoptions(precision=8, suppress=True, linewidth=120) #prinintg from numpy\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58378b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7243df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# # Create some tensors\n",
    "# a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "# b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "# c = tf.matmul(a, b)\n",
    "\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f18bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fa7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[1], cmap='gray_r') \n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34773cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,11):\n",
    "#     plt.subplot(1,10,i)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(X_train[i-1], cmap='gray_r')\n",
    "#     plt.title(y_train[i-1], color='black', fontsize=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e125cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUR MODELS\n",
    "\n",
    "#SMALL MODEL\n",
    "model_small = Sequential()\n",
    "model_small.add(Flatten(input_shape=(28,28)))\n",
    "model_small.add(Dense(units=8, activation='relu'))\n",
    "model_small.add(Dropout(0.2))#20% of data for dropout\n",
    "model_small.add(Dense(units=10, activation='softmax')) #output\n",
    "\n",
    "model_small.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_small.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trening- fit ! \n",
    "history_small = model_small.fit(X_train, y_train, epochs=30, batch_size=50, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1633fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_small=pd.DataFrame(history_small.history)\n",
    "metrics_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8830935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUR MODELS\n",
    "\n",
    "#BASIC MODEL\n",
    "model_basic = Sequential()\n",
    "model_basic.add(Flatten(input_shape=(28,28)))\n",
    "model_basic.add(Dense(units=128, activation='relu'))\n",
    "model_basic.add(Dropout(0.2))#20% of data for dropout\n",
    "model_basic.add(Dense(units=10, activation='softmax')) #output\n",
    "\n",
    "model_basic.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_basic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f51ab3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model trening- fit ! \n",
    "history_basic = model_basic.fit(X_train, y_train, epochs=30, batch_size=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_basic=pd.DataFrame(history_basic.history)\n",
    "metrics_basic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7424b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_test_basic, accuracy_test_basic =  model_big.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUR MODELS\n",
    "\n",
    "#BIG MODEL\n",
    "model_big = Sequential()\n",
    "model_big.add(Flatten(input_shape=(28,28)))\n",
    "model_big.add(Dense(units=256, activation='relu'))\n",
    "model_big.add(Dense(units=512, activation='relu'))\n",
    "model_big.add(Dropout(0.2))#20% of data for dropout on reg.\n",
    "model_big.add(Dense(units=10, activation='softmax')) #output\n",
    "\n",
    "model_big.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_big.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_big = model_big.fit(X_train, y_train, epochs=30, batch_size=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce17e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_big=pd.DataFrame(history_big.history)\n",
    "metrics_big.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b23fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Three models and its accuracy/ loss plots below. As we can see our big model is doing almost as good as the basic one if we look at the accuracy.\n",
    "## It means that we don`t need that kind of complex architecture. Loss and accuracy for our small model may be a little weak for us.\n",
    "## Big model overfits very quickly \n",
    "\n",
    "## SMALL MODEL - one hidden layer with 8 units\n",
    "## BASIC MODEL - one hidden layer with 128 units\n",
    "## BIG MODEL - two hidden layer with 512/256 units\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=metrics_small['loss'],name='loss_small'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=metrics_small['accuracy'],name='accuracy_small'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(y=metrics_small['val_loss'],name='val_loss_small'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=metrics_small['val_accuracy'],name='val_accuracy_small'), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=metrics_basic['loss'],name='loss_basic'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=metrics_basic['accuracy'],name='accuracy_basic'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(y=metrics_basic['val_loss'],name='val_loss_basic'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=metrics_basic['val_accuracy'],name='val_accuracy_basic'), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=metrics_big['loss'],name='loss_big'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=metrics_big['accuracy'],name='accuracy_big'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(y=metrics_big['val_loss'],name='val_loss_big'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=metrics_big['val_accuracy'],name='val_accuracy_big'), row=2, col=1)\n",
    "\n",
    "fig.update_layout(width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eeb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We could try to find \"the best models\" using checkpoint and earlystopping. Lets find out how many epochs is enough to get the best model out from basic and the big model\n",
    "## We use vaL_accuracy as monitor and 5 epochs for earlystopping and saving weights to the file\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# basic model\n",
    "filepath ='best_basic_model_weights.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', save_best_only=True, verbose=1, mode='max')\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose = 1, patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "history__best_basic = model_basic.fit(X_train, y_train, epochs=30, batch_size=50, validation_split=0.2, callbacks=[checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST BASIC MODEL - loading weights from the file\n",
    "\n",
    "best_model_basic = Sequential()\n",
    "best_model_basic.add(Flatten(input_shape=(28,28)))\n",
    "best_model_basic.add(Dense(units=128, activation='relu'))\n",
    "best_model_basic.add(Dropout(0.2))#20% of data for dropout\n",
    "best_model_basic.add(Dense(units=10, activation='softmax')) #output\n",
    "\n",
    "best_model_basic.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "best_model_basic.load_weights('best_basic_model_weights.hdf5')\n",
    "\n",
    "best_model_basic.summary()\n",
    "best_model_basic.save('my_best_basic_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add22bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_basic, accuracy_basic =  model_basic.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smth wrong ?! accuracu 0.0983 ? \n",
    "\n",
    "loss_best_model_basic, accuracy_best_model_basic =  best_model_basic.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dbdffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#big  model\n",
    "\n",
    "filepath ='best_big_model_weights.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose = 1, patience = 5)\n",
    "history__best_big = model_big.fit(X_train, y_train, epochs=30, batch_size=50, validation_split=0.2, callbacks=[checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04198a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST BIG MODEL - loading weights from the file\n",
    "\n",
    "best_model_big = Sequential()\n",
    "best_model_big.add(Flatten(input_shape=(28,28)))\n",
    "best_model_big.add(Dense(units=256, activation='relu'))\n",
    "best_model_big.add(Dense(units=512, activation='relu'))\n",
    "best_model_big.add(Dropout(0.2))#20% of data for dropout on reg.\n",
    "best_model_big.add(Dense(units=10, activation='softmax')) #output\n",
    "\n",
    "best_model_big.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "best_model_big.load_weights('best_big_model_weights.hdf5')\n",
    "\n",
    "best_model_big.save('my_best_big_model.h5')\n",
    "best_model_big.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
